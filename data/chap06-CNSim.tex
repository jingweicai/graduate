% !TeX root = ../thesis.tex

\chapter{Cycle-Accurate Packet-Parallel Simulator for Large-Scale Chiplet Networks}
\label{chap06:cnsim}

Chiplet-based systems are significantly different from traditional systems, and the chiplet interconnection architecture has not been thoroughly evaluated. Critical design issues to be evaluated include but are not limited to: \textbf{1)} hierarchical topologies of the on-chiplet and off-chiplet networks, \textbf{2)} routing algorithms on the heterogeneous networks, \textbf{3)} impacts of the heterogeneous chiplet-to-chiplet interfaces and heterogeneous routers, and \textbf{4)} overall performance of various topologies under various workloads. There are two main challenges in evaluating chiplet-based interconnection networks.

\textbf{Challenge 1.} Unified simulations on the entire network are necessary for evaluating chiplet interconnection architecture. Traditional on-chip and off-chip networks are usually separately designed. On-chip networks are connected to a switch, and the off-chip network is constructed among multiple switches regardless of the on-chip network architecture. However, the \textit{Chiplet} architecture integrates multiple silicon dies with ultra-high density and connectivity, breaking the boundary between on-chip and off-chip networks. Multiple on-chip routers of chiplets are directly connected by low-latency off-chip links, forming large-scale heterogeneous networks and leading to potential problems: \textbf{1)} Networks-of-chiplet cannot be separately evaluated since they are tightly coupled with on-chiplet networks~\cite{Yin_ModularRoutingDesign_2018}. \textbf{2)} The scale of chiplet-based networks can be much larger than a traditional on-chip network~\cite{Chang_DOJOSuperComputeSystem_2022}. \textbf{3)} The on/off-chip links and routers are heterogeneous in mechanism, bandwidth, and latency, which are not uniformly and accurately modeled in existing tools.

\textbf{Challenge 2.} Existing network simulators are inefficient for evaluating large-scale chiplet-based shared-memory networks. On-chip and off-chip networks have been two distinct areas, each with abundant existing evaluation tools. Due to the latency of traditional off-chip links and switches, \textit{e.g. Ethernet} and \textit{InfiniBand}, is usually in the order of microsecond (us)~\cite{Sella_FECKilledCutThrough_2018,Katebzadeh_EvaluationInfiniBandSwitch_2020}, a coarse-grained event-based simulator is sufficient for evaluating traditional off-chip networks. However, low-latency on-chip routers in multi-chiplet systems are directly connected by low-latency links of nanoseconds (ns), \textit{e.g. UCIe}~\cite{_UniversalChipletInterconnect_2023}. Besides, circuit-level microarchitecture features, including buffer, link, switch, and flow-control, significantly impact the performance of chiplet-based systems. Therefore, it is necessary to use fine-grained cycle-accurate simulators to evaluate chiplet-based networks. However, existing cycle-accurate tools are inefficient for large-scale chiplet-based networks because they are designed for small-scale on-chip networks~\cite{Jiang_DetailedFlexibleCycleaccurate_2013,Agarwal_GARNETDetailedOnchip_2009,Ben-Itzhak_HNOCSModularOpensource_2012,Catania_NoximOpenExtensible_2015}, whose scale is no more than tens of routers. Existing parallel network simulators are designed for MPI-based distributed-memory systems~\cite{Rodrigues_StructuralSimulationToolkit_2011,Carothers_ROSSHighperformanceLowmemory_2002}; thus, they are also unsuitable for chiplet-based shared-memory systems~\cite{Besta_MillionServerNetworkSimulations_2021}.

Existing cycle-accurate network simulators are slow for two main reasons: \textbf{1)} The simulator models the entire RTL router, including many noncritical circuit behaviors, \textit{e.g.} hand-shake and the \textit{round-robin} allocation. \textbf{2)} Complex resource dependency and competition (thread synchronization) make network simulation challenging to parallelize. Therefore, for the first reason, people are motivated to compromise some unnecessary modeling to accelerate simulation speed while verifying necessary microarchitectures and maintaining cycle accuracy. For the second reason, the atomic-based multi-threading mechanism can realize efficient packet-parallel simulation. As a result, \textit{Chiplet Network Simulator (CNSim)} is built, which is a cycle-accurate packet-parallel network simulator that helps us to evaluate the chiplet interconnection architecture systematically and efficiently. The contributions of this chapter are summarized as follows:
\begin{itemize}
    \item Unlike existing small-scale on-chip or large-scale distributed network simulators, \textit{CNSim} is designed for large-scale chiplet-based (shared-memory) networks.
    \item A packet-centric simulation architecture and an atomic-based multi-threading mechanism are adopted. Simulation speed is $11\times \sim 14\times$ faster than existing tools while verifying necessary microarchitectures and maintaining cycle accuracy.
    \item Various features for evaluating chiplet-based networks are supported, including heterogeneous router/link, hierarchical topology, adaptive routing, and real workload traces integration. The simulator and the evaluation framework are open-sourced to the community~\footnote{https://github.com/Yinxiao-Feng/chiplet-actuary}.
    \item Based on \textit{CNSim}, two typical chiplet-based networks, which
    cannot be efficiently simulated by existing simulators, are systematically evaluated. Extensive evaluations reveal the advantages and limitations of the chiplet interconnection architecture.
\end{itemize}

\section{Motivation}
\subsection{Difference Between Traditional and Chiplet-Based Networks} 

\textbf{Throughput:} An obvious limitation of traditional interconnection architectures is that the NICs or I/O routers are the bottleneck of the network. The traffic that can be injected into the off-chip network is much less than the total throughput of the on-chip network. For example, two servers (processors) are connected to a 64-port 400G switch, with a total switching bandwidth of 25.6Tb/s; however, the communication bandwidth between the two servers is only 400 G. For some workloads (\textit{e.g.} AI), the local throughput is supposed to be higher than global throughput~\cite{Hoefler_HammingMeshNetworkTopology_2022}. By advanced packaging technologies and high-speed wireline technologies, the chip itself can provide communication bandwidth no weaker than a regular switch~\cite{Fischer_91D17nm_2023, Hoefler_HammingMeshNetworkTopology_2022}; thus, the injection/ejection bandwidth of each chiplet is no longer bounded by the centralized NIC or I/O router, significantly improving the network scalability and throughput.

\textbf{Latency:} The latency of chiplet components is much lower than the traditional off-chip switches and links, \textit{e.g.} Ethernet~\cite{Sella_FECKilledCutThrough_2018} and InfiniBand\cite{Katebzadeh_EvaluationInfiniBandSwitch_2020}, whose latencies are more than hundreds of nanoseconds (us-level). However, the latency of typical on-chip routers and chiplet-to-chiplet interfaces, \textit{e.g.} UCIe~\cite{_UniversalChipletInterconnect_2023}, is only a few nanoseconds (cycle-level). As a result, the microarchitecture of on-chip routers can significantly affect the overall performance of chiplet-based systems. On the other hand, the low-latency die-to-die links make replacing a costly switch-to-switch hop with multiple low-cost chiplet-to-chiplet hops possible, which makes the chiplet-based large-scale network have a diameter of numerous hops. Compared with traditional switch-based high-radix networks, chiplet-based networks require cycle-accurate modeling to evaluate the overall performance; however, such fine-grained simulations of large-scale networks can be slow.

\textbf{Shared-memory vs. distributed-memory:} In a shared-memory architecture, multiple processors or cores share a common address space, while in a distributed-memory architecture, each processor has its own private memory space. Shared-memory architecture is programming-friendly, but the hardware-based coherence protocol has high requirements for network performance; thus, current large-scale systems are usually distributed-memory-based. However, the high connectivity and low latency of chiplet architecture make large-scale chiplet-based systems adopting the shared-memory architecture possible~\cite{Gomes_PonteVecchioMultiTile_2022}. As a result, the MPI-based parallelism mechanism, used in most distributed-memory systems, is unsuitable for chiplet-based systems. The shared-memory architecture presents simulation challenges for large-scale chiplet-based networks due to the lack of a parallel simulation framework.
\subsection{Network Simulators}
\label{chap06:sec:network-simulators}
Various existing network simulators can be categorized into \textit{cycle-based} and \textit{discrete-event} according to the simulation mechanism.
The \textbf{cycle-based simulator} updates the states of all components in each cycle based on the states of the last cycle. The popular \textit{BookSim}~\cite{Jiang_DetailedFlexibleCycleaccurate_2013} and other cycle-accurate simulators~\cite{Catania_NoximOpenExtensible_2015, Lis_DARSIMParallelCyclelevel_2010} adopt such a design because it is highly compatible to the actual implementation of routers. Cycle-based simulators are capable of modeling fine-grained microarchitectures but are not qualified for evaluating large-scale distributed systems without a synchronous clock.
The \textbf{discrete-event simulator} is a more mainstream approach for evaluation large-scale networks~\cite{Agarwal_GARNETDetailedOnchip_2009, _OMNeTDiscreteEvent_,_Ns3NetworkSimulator_, Rodrigues_StructuralSimulationToolkit_2011, Carothers_ROSSHighperformanceLowmemory_2002}. Any change in discrete-event simulators is identified as an event, and the simulator processes every event along the timeline. The discrete-event simulator can also be cycle-accurate~\cite{Ben-Itzhak_HNOCSModularOpensource_2012} by generating events at cycle-granularity, but the simulation can be slow due to the serial processing of the large event queue.

\textbf{Parallel Simulation:} Since traditional on-chip networks are scale-limited, existing cycle-accurate simulators pay little attention to simulation performance. In fact, they are already quite time-consuming for on-chip networks (\textit{e.g.}, it takes tens of hours to simulate the entire \textit{PARSEC} traces~\cite{Hestness_NetraceDependencydrivenTracebased_2010,Hestness_NetraceDependencyTrackingTraces_2011}), let alone for large-scale networks. For large-scale distributed networks, almost all simulators are based on parallel discrete-event simulation (PDES)~\cite{Fujimoto_ParallelDiscreteEvent_1990}. The system is partitioned into separate simulation objects, each with its own event queue and \textit{Logical Process (LP)}. Necessary synchronizations are performed among different LPs to guarantee that the distributed events are processed appropriately. In most existing parallel discrete-event simulators, including \textit{SST}~\cite{Rodrigues_StructuralSimulationToolkit_2011}, \textit{ROSS/CODES}~\cite{Carothers_ROSSHighperformanceLowmemory_2002,Mubarak_EnablingParallelSimulation_2017}, \textit{NS-3}~\cite{_Ns3NetworkSimulator_}, and \textit{OMNeT++}~\cite{_OMNeTDiscreteEvent_}, systems are partitioned into MPI ranks/nodes, and the synchronization is achieved by MPI communication. However, the distributed-memory-based clusters are unavailable to most researchers~\cite{Besta_MillionServerNetworkSimulations_2021}, and the MPI-based mechanism is unsuitable for chiplet-based shared-memory systems. Therefore, multi-threading is more suitable for parallel-simulating large-scale chiplet-based networks. As with all multithreaded programs, the multi-threading bottleneck is the program's parallelizability and the synchronization overhead between threads. Packets and resources in a network are tightly dependent and associated, thus challenging to parallelize.

\textbf{Parallelism strategy:} Numerous components in a network can be parallelized in different ways. The \textit{network parallelism}, in which multiple threads concurrently process multiple subnetworks/routers, is commonly used~\cite{Lis_DARSIMParallelCyclelevel_2010}. Although \textit{network parallelism} is straightforward, there are two major limitations. First, each router (thread) has resource dependency and contention with all adjacent routers, which leads to fierce thread competition. Second, multiple physical/virtual channels with multiple packets in a router are serially processed, which limits the parallelism for high-radix and large-buffer routers. Therefore, we are motivated to propose a new strategy, called \textit{packet parallelism}, promising to achieve more efficient parallel simulation.

% Two synchronization mechanisms are widely used to guarantee the distributed events are performed appropriately: \textbf{1)} If the evaluated traffic is message-passing-interface-based (MPI-based), synchronization can be naturally achieved through the MPI communication protocol. \textbf{2)} If the network traffic has no built-in synchronization mechanism, manual synchronization is required, which is challenging to balance speed and accuracy. Multi-chiplet systems usually adopt shared-memory architecture without distributed features; that is to say, the simulator must manually handle all synchronization to achieve parallel simulation. Unfortunately, existing parallel simulators only apply to MPI-based distributed systems, and few cycle-accurate parallel simulators are available for large-scale shared-memory systems.
\subsection{Motivations from Profiling BookSim}
\label{chap06:sec:motivation-allocation}

For real circuit-level routers, every pipeline stage is finished in one cycle, no matter whether it is critical or trivial. However, for simulators, the runtime of different stages can vary greatly. The simulation model of an unimportant circuit stage can cost a lot of time. Besides, the computer's memory capacity for running the simulation is finite; therefore, simulators can't fully replicate the hardware mechanism, \textit{e.g.} the routing table, which leads to potential simulation overheads. As a result, the simulation runtime and circuit runtime are disproportionate. To fully demonstrate why existing cycle-accurate simulators are slow, the \textit{BookSim} is profiled as an example. The \textit{Valgrind Callgrind} framework~\cite{_ValgrindHome_}, which can count the execution cycles of function calls, is used for profiling.

\begin{table}[tb]
    \centering
    \caption{Profiling results, measured by cycles of function calls, of the \textit{BookSim} on the 2D-mesh and dragonfly topologies. \label{table:profiling}}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Cycle Estimation (\%)}                   & \textbf{2D-Mesh} & \textbf{Dragonfly} \\ 
        \midrule
        \texttt{IQRouter::\_internalStep()}     & 92.18   & 71.92     \\
        \quad Call: \texttt{\_VCAllocEvaluate()}      & 48.08   & 17.65     \\
        \quad Call: \texttt{SparseAllocator::Clear()} & 28.86   & 17.93     \\
        \quad Call: \texttt{\_SWAllocEvaluate()}      & 5.71    & 13.43     \\
        \quad Call: \texttt{\_SWAllocUpdate()}        & 4.59    & 11.26     \\ 
        \bottomrule
    \end{tabular}
\end{table}

As shown in Table~\ref{table:profiling}, the router status update function \texttt{IQRouter::\_InternalStep()}, which is called every simulation cycle by every router, takes up the majority of the runtime. Further decomposition shows that most of the runtime is spent processing resource allocation, including the virtual channel and switch. In the process, a request mapping table \texttt{vector<vector<sRequest>>} for ``requester-provider'' pairs is maintained to determine the allocation of resources in a \textit{round-robin} way. The frequent indexing, erasing, and inserting of the table significantly affect the simulation performance. Specifically, more than $70\%$ of the time is spent evaluating the allocation behavior, which is an already thoroughly discussed issue. As a matter of fact, different allocation policies, including \textit{round-robin} and \textit{first-come-first-serve (FCFS)}, have advantages and disadvantages, but they do not seriously affect the overall performance because either policy fully utilizes the physical channel~\cite{Fischer_AccurateScalableAnalytic_2013, Agarwal_SurveyNetworkChip_2009}.

Since allocation policy is not a significant issue to be evaluated, saving related simulation times for evaluating large-scale chiplet-based networks is necessary. Therefore, we are motivated to use an implementation-friendly allocation policy and a more efficient simulation mechanism. In brief, the simulation speed can be significantly improved if the available resources can be immediately allocated to the requester without gathering and managing all the requests. If all the requests are processed in the coming order, the \textit{first-come-first-serve} policy is naturally achieved. Maintaining numerous coming-order queues is difficult, but a packet queue can be easily maintained based on the injection order, \textit{i.e.} the \textit{first-inject-first-serve} policy.



\section{Cycle-Accurate Packet-Parallel Simulator Architecture}

The \textit{Chiplet Network Simulator (CNSim)} is a cycle-accurate simulator designed for large-scale chiplet-based (shared-memory) interconnection networks but also applies to traditional on/off-chip networks. This chapter mainly focuses on the novel features of \textit{CNSim}; other features can be found in the source code and appendix.

\subsection{Packet-Centric Simulation}
In cycle-based simulators, all state elements of the network, including buffer/link usages, requests, and routing/allocation results, are updated in each cycle. Such a mechanism is compatible with the circuit implementation and implies a natural clock-level synchronization. In contrast, an event queue is maintained in discrete-event simulators, and the simulator processes discrete events along the timeline, which is more general and flexible. As shown in Figure~\ref{chap06:fig:simulator}, we present a new \textit{packet-centric} architecture, combining the advantages of cycle-based and discrete-event simulators. A packet queue is maintained based on the injection time, and all packets are updated every cycle sequentially. The status values stored in each packet include the routing results, the allocated buffer, the switch allocation status, the trace of each flit, and necessary statistical information. As shown in Algorithm~\ref{alg:update-pkt}, each packet is updated once per cycle according to the network status. Each update is equivalent to processing a series of events in the discrete-event simulator, including routing, VC allocation, switch allocation, link traversal, and flit transmission.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/2024ATC/simulator.pdf}
    \caption{Overview of the packet-centric \textit{CNSim}. The key status values are stored in \texttt{Packet} rather than in the network, and each packet is updated once per cycle according to the network status. \label{chap06:fig:simulator}}
\end{figure}

\begin{figure}[htb]
\centering
\begin{minipage}{.6\linewidth}
\begin{algorithm}[H]
    \caption{\scshape Processing Packet \label{alg:update-pkt}}
    \begin{algorithmic}[1]
        \REQUIRE The packet $p$;
        \IF{$p$.candidate\_channels is EMPTY}
        \STATE \scshape{Routing}($p$);
        \ELSIF{$p$.allocated\_buffer is NULL}
        \STATE \scshape{VC\_Allocation}($p$);
        \ELSIF{$p$.switch\_allocated is FALSE}
        \STATE \scshape{Switch\_Allocation}($p$);
        \ENDIF
        \IF{$p$.switch\_allocated is TRUE}
        \STATE \scshape{Link\_Traversal}($p$.head);
        \ENDIF
        \STATE Transmit all body flits one step forward.
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure}

As introduced in Section~\ref{chap06:sec:motivation-allocation}, the allocation modeling is supposed to be improved. The \textit{packet-centric} architecture can directly and efficiently achieve the \textit{first-inject-first-serve (FIFS)} policy. As shown in Figure~\ref{chap06:fig:simulator}, packets injected by the \textit{traffic manager} are appended to the end of the packet container. If a packet reaches the destination, it is removed without affecting the order of the remaining packets. Since the single-thread simulator processes the packet queue sequentially in each cycle, if the available resources are directly allocated to the request, \textit{FIFS} allocation policy is naturally achieved. With such a mechanism, the costly management of the complicated request-resource mapping is eliminated, and the simulation speed can be significantly improved. Besides the \textit{FIFS} policy, \textit{FCFS} can also be easily implemented by maintaining a packet queue per router.

However, utterly real-time resource management can lead to process-order-induced deviations. For example, suppose an earlier packet directly releases the resources it occupies (\textit{e.g.} physical link). In that case, the later packet may successfully acquire the resource in the same cycle, which violates the hardware mechanism. Therefore, the critical resources are still recorded (\textit{e.g.} physical link) and update the status of these resources after all packets have been processed. Such a scheme can be regarded as cycle-level synchronization, which is necessary to ensure the simulation accuracy. The packet-centric architecture and the cycle-level synchronization are compatible with the multi-threading simulation.

\subsection{Packet Parallelism Simulation}
\label{chap06:sec:packet-parallel}
Concurrency can significantly accelerate the simulation; however, it is not easy for simulators to achieve efficient parallel simulation. As discussed in Section~\ref{chap06:sec:network-simulators}, the commonly-used \textit{network-parallelism} has limitations; thus, we present a new \textit{packet-parallelism} to process packets concurrently.

As shown in Figure~\ref{chap06:fig:packet-parallel}, the packet queue is regarded as a workload pool shared by all worker threads. Each worker thread sequentially fetches one or a few packets from the queue for processing.  After a round of parallel processing, status updates have been completed for most of the packets, but some packets that modify critical resources, including buffers and links, are tagged with special status. Then, the main thread handles all necessary synchronization, including updating the link status and removing finished packets. One major advantage of \textit{packet parallelism} is that a packet is less likely to compete for resources with other packets. For example, only the head packet of an input queue competes with the head packets of other input queues in the same router. Therefore, the overheads and deviations caused by multi-threading can be negligible. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/2024ATC/packet-parallel.pdf}
    \caption{The packet-parallel scheme. Each worker fetches a packet from the queue for processing sequentially. The earlier processed packet directly gets the allocation regardless of later requests, \textit{i.e.} approximate \textit{first-inject-first-serve}. \label{chap06:fig:packet-parallel}}
\end{figure}

\textbf{Atomic-based parallel programming.} Synchronization is the most important but performance-costly operation for multithreading programs. Traditional parallel simulators~\cite{Lis_DARSIMParallelCyclelevel_2010} manually handle thread contentions using \textit{locks} in any critical section. For example, if \textit{packet parallel} is used, the link/switch allocation must wait until all packet requests of all threads have been collected to determine the final result. Such synchronization seriously affects the performance of parallel simulation. Therefore, we adopt an atomic-based mechanism that sacrifices some allocation consistency to avoid frequent synchronization and accelerate parallel simulation. Shared resources, including buffers and links, are set to atomic variables, and the first packet of a thread that successfully modifies the atomic variable wins the contention. The thread contention decides the allocation of resources instead of the manual handling logic. As shown in Figure~\ref{chap06:fig:packet-parallel}, the switch allocation status is an atomic variable and is directly allocated to the first thread acquiring it, regardless of the requests of other threads. 

\textbf{Multi-thread inconsistency.} Compared with the single-thread simulation, the multi-thread program no longer follows the strict first-injected-first-served policy but an approximate one with multi-thread randomness that later-injected packets may win the allocation in the competition with earlier-injected packets. Besides, the uncertain acquiring/releasing order for the buffer can also lead to one-flit wasted buffer space. However, since all packets are dispatched to workers (threads) in injection order, early packets are still more likely to win the contention. Validation experiments in Section~\ref{chap06:sec:inconsistency} also show negligible inconsistency after parallelism. \textit{Negligible} is defined as the average packet latency deviations caused by multi-threading are smaller than the deviations of different random seeds for the traffic pattern, which is measured at $0.5\%$. Another potential problem is that such a mechanism may lead to starvation and affect performance. However, permanent starvation is unlikely because the thread contention is random. 


The \textit{packet parallelism} can be combined with the \textit{network parallelism}. For the \textit{first-come-first-serve} allocation policy, each router maintains a packet queue, and workers can process packets in different queues concurrently. When too many threads are in parallel, competition among workers for the next workload (packet) in each queue becomes intense. To prevent the head-packet competition from becoming the bottleneck, each worker can fetch multiple packets from the queue every time. However, such a ``multiple-issue'' scheme may lead to worse allocation inconsistency; thus,  validation experiments Section~\ref{chap06:sec:inconsistency} is to verify the efficiency-accuracy balance.

\subsection{Heterogeneous Router \& Link}
In traditional on-chip networks, all links are uniform (one flit/cycle); therefore, all routers and links in existing cycle-accurate network simulators are uniformly configured with the same latency and bandwidth. However, the inter-chiplet links are different from the on-chip links, and a chiplet can have multiple types of off-chiplet interfaces. Besides, due to the heterogeneity of links, the routers on the chiplet are also heterogeneous. Heterogeneity is a significant feature that all existing cycle-accurate simulators lack in modeling.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/2024ATC/heterogeneous.pdf}
    \caption{Unified modeling of the heterogeneous routers and links. (a) Multiple flits at the head of the input buffer can be transmitted to the next router in one cycle; (b) $n$ virtual pipeline stages are used to simulate a channel of $n$ cycles latency. \label{chap06:fig:heterogeneous}}
\end{figure}

In \textit{CNSim}, all different kinds of routers and links are built by a uniform model, and objects are configured individually in the simulation. As shown in Figure~\ref{chap06:fig:heterogeneous}, the \textit{multi-width FIFO} and \textit{virtual pipeline stages} are implemented to model heterogeneous routers and links uniformly. For a link of $m$ flits/cycle bandwidth and $n$ cycle latency, at most $m$ flits at the head of the input buffer can be transmitted to the next router. It takes $n$ virtual pipeline stages in total to finish the transmission. Non-integer delays are directly approximated as integers, and non-integer bandwidths can be achieved by alternately passing integer numbers of flits, \textit{e.g.} $1.5$ flits/cycle bandwidth can be achieved by passing $2$ flit in the first cycle and $1$ flits in the next cycle. Besides, \textit{CNSim} is also compatible with the two newly proposed heterogeneous interface implementations, \textit{hetero-PHY} and \textit{hetero-channel}.

\subsection{Other Notable Features \& Compromises}
\textbf{Eliminate unimportant pipeline stages:} As shown in Figure~\ref{chap06:fig:pipelines}, a typical cycle-accurate network simulator usually models all circuit-level pipelines of routers, including input queuing (IQ), routing computation (RC), virtual channel allocation (VA), switch allocation (SA), switch traversal (ST), and link traversal (LT).
Among these stages, what really affects the overall performance are those competing for resources and causing pipeline stalls. For example, the switch allocation to one input port stalls other input ports requesting the same output port. To accelerate the simulation, we focus only on four core stages: RC, VA, SA, and LT. Other router stages are eliminated because they don't cause pipeline stalls. For example, a packet allocated to the output port is immediately transmitted through the link rather than going through switch traversal and output queuing. Smooth transmission is guaranteed by the VC/switch/link allocation.

\textbf{Cache the results of repetitive routing computations:} Routing computation is a time-consuming stage in simulation, especially for adaptive routing and complex hierarchical topologies. Using a complete routing table is impossible because the table size of an $n$ nodes network can be $O(n^2)$, which is too large for memory. As a compromise, we cache some results of repetitive computations. Taking the \textit{Dragonfly} topology as an example, we cache the global channel between any two groups and the local channel between any two routers in the same group. By caching these results, though the memory usage is increased, the simulation speed can be significantly improved. Validation experiments in Section~\ref{chap06:sec:simulation-speed} show that the memory usage overhead of \textit{partial caching} is not significant.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.7\linewidth]{../figures/2024ATC/pipelines.pdf}
    \caption{Some pipeline stages of a router don't cause stalls and thus can be eliminated for accelerating simulation. \label{chap06:fig:pipelines}}
\end{figure}

\textbf{Integration with real workload traces:} \textit{CNSim} supports various traffic patterns, including the permutation patterns and Ring-AllReduce patterns. To better analyze the network performance under real workloads, \textit{CNSim} also supports two typical traces. First is the shared-memory workload traces with cycle-accurate timestamps. The \textit{Netrace} are collected from a 64-core M5 simulation system~\cite{Binkert_M5SimulatorModeling_2006} executing multithreaded applications from the PARSEC v2.1 suite~\cite{Bienia_PARSECBenchmarkSuite_2008, Hestness_NetraceDependencyTrackingTraces_2011, Hestness_NetraceDependencydrivenTracebased_2010}. Each trace item records the \texttt{inject\_cycle}, \texttt{bus\_name}, \texttt{src\_port\_id}, \texttt{dst\_port\_id}, and \texttt{msg\_type} (message size). By using cycle-accurate traces, the evaluation results are more persuasive. \textit{CNSim} also supports the distributed-memory workload traces without timestamps. The traces include a list of messages (source-destination) by order of generation~\cite{Avin_ComplexityTrafficTraces_2020}. The performance of distributed-memory traces can be evaluated by adjusting the injection rate.

\section{Validation and Evaluation}
\subsection{Implementation \& Performance Simulation}
\textit{CNSim} is implemented in C++ and validated by comparison with three typical cycle-accurate simulators: \textit{BookSim}~\cite{Jiang_DetailedFlexibleCycleaccurate_2013},  \textit{Garnet}~\cite{Agarwal_GARNETDetailedOnchip_2009}, and \textit{OMNET++}~\cite{_OMNeTDiscreteEvent_}. The release versions of these simulators in the experiments are the \textit{BookSim} 2.0, \textit{HNOCS}~\cite{Ben-Itzhak_HNOCSModularOpensource_2012} based on OMNeT++ 5.7.1, and Garnet 3.0~\cite{Bharadwaj_KiteFamilyHeterogeneous_2020} based on Gem5 23.0. The \textit{HNOCS} is compiled by gcc/g++ 9.4, and all other simulators are compiled by gcc/g++~11.4. Most compilation options are left at default, and the ``O2/O3'' optimizations are turned on when possible in comparing runtime. All the experiments are performed on an Intel Core I9-13900K CPU with 32 GB of two-channel memory.

The router microarchitecture implemented in the \textit{CNSim} is the typical input-queued virtual channel router, similar to most other cycle-accurate simulators. However, implementation details can vary: \textit{e.g.}, Garnet is two-stage pipelined; BookSim is four-stage pipelined; OMNeT++ is event-driven and thus does not have a cycle-based pipeline; \textit{CNSim} can be one-stage or multi-stage by different configurations. The performance of the networks in different simulators is evaluated through latency-injection curves. Dimensional order (\textit{i.e.} XY) routing on 2D-mesh topology is adopted, and the network scale is $4\times4$ and $8\times8$. Critical parameters, including the packet size, buffer size, and VC number, are set to $5$-flits, $4$-packets, and $2$-VC; other parameters remain default. The evaluated traffic is uniformly random, and all experiments are simulated for 10K cycles.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/2024ATC/simulators-latency.pdf}
    \caption{The latency-injection curves of different cycle-accurate simulators for uniform traffic on 2D-mesh. \label{chap06:fig:simulators-latency}}
\end{figure}

All these cycle-accurate simulators can be used for evaluating microarchitecture and overall performance, but the performance results of different simulators are different due to the diverse modeling. As shown in Figure~\ref{chap06:fig:simulators-latency}, the simulated performance of \textit{CNSim} is close to \textit{Garnet} and better than the \textit{BookSim} because \textit{BookSim} is four-stage pipelined, and it strictly simulates the circuit implementation. \textit{CNSim} and \textit{Garnet} use a more efficient router design but do not reflect some circuit-level behaviors. The OMNeT-based \textit{HNOCS} is cycle-accurate but not cycle-based; as a result, it behaves differently from other simulators.

\subsection{Simulation Speed \& Memory Usage}
\label{chap06:sec:simulation-speed}

One of the significant advantages of \textit{CNSim} is the simulation speed. The simulation performance of \textit{CNSim} is compared with other simulators. Each experiment uses uniform traffic, continues for 100K cycles, and the runtime is measured. As shown in Figure~\ref{chap06:fig:sim-speed}(a), the single thread \textit{CNSim} is over $14\times$ faster than existing cycle-accurate simulators for the $4\times4$ 2D-mesh topology. For small-scale networks, multi-threading is not necessary. As shown in Figure~\ref{chap06:fig:sim-speed}(b), single thread \textit{CNSim} is $\sim 4\times$ faster than \textit{BookSim} for the large-scale \textit{Dragonfly} of 16K nodes. The 8-threads \textit{CNSim} achieves $~3\times$ multi-threading speedup and is $\sim 11\times$ faster than the \textit{BoosSim}. The runtime comparisons show that \textit{CNSim} is much more efficient than existing cycle-accurate simulators under various conditions.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{../figures/2024ATC/sim-speed.pdf}
    \caption{Simulation run time comparison for different simulators and topologies. \label{chap06:fig:sim-speed}}
\end{figure}

The computation complexity can be estimated by $O(nIT\widetilde{L})$, where $n$ is the network scale (router number), $I$ is the injection rate (flits/cycle/router), $T$ is the simulation time (cycles), and $\widetilde{L}$ is the average latency (cycles) of packets. As a result, the speedup can be fewer when the network scale is ultra-large and heavily congested since $\widetilde{L}$ is large (more than hundreds of cycles) in such simulations. However, \textit{CNSim} can still be much faster than other simulators by using enabling multi-threading. More evaluation of parallelism is presented in the next subsection.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/2024ATC/memory.pdf}
    \caption{Comparison of heap memory consumption at different injection rates. The two other simulators are based on other frameworks (very heavy Gem5 and OMNeT++); therefore, it is hard to evaluate the memory consumption separately and fairly. So, they are not compared in the picture. \label{chap06:fig:memory}}
\end{figure}

Another factor affecting a simulator's scalability is the memory overhead, especially for multi-threading simulators running on a single machine. We profile the heap memory usage of \textit{CNSim} using \textit{Valgrind Massif}~\cite{_ValgrindHome_}. Uniform traffic pattern on 16K nodes \textit{Dragonfly} is simulated. As shown in Figure~\ref{chap06:fig:memory}, \textit{CNSim} consumes 131.7~MB to 297.5~MB heap memory at 0.1 to 0.6 flits/cycle/node injection rates. For comparison, the \textit{BookSim} consumes 586.2~MB to 1.1~GB heap memory at 0.1 to 0.6 injection rates. Since \textit{CNSim} is packet-centric, the \texttt{Packet} objects, tracing records, and routing results consume much memory, and the consumption is proportional to the packet number. Besides, the network status and the cached partial routing tables also take up some fixed-size memory.
When the network is heavily congested (0.6 flits/cycle/node in Figure~\ref{chap06:fig:memory}), the memory usage of \textit{CNSim} will be higher because the packet number is significant. Even so, the memory usage of \textit{CNSim} is still much lower than \textit{BookSim} because \textit{CNSim} does not maintain request mapping tables. In conclusion, the packet-centric architecture and routing caching technology do not bring memory overhead, and \textit{CNSim} is scalable for large-scale networks.

\subsection{Concurrency \& Inconsistency}
\label{chap06:sec:inconsistency}
\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{../figures/2024ATC/multi-threads.pdf}
    \caption{Speedup of parallel simulations with different number of threads.  \label{chap06:fig:multi-threads}}
\end{figure}

\textit{CNSim} achieves parallel (multi-threading) simulation on any multicore CPU without additional programming overheads. The parallel speedup of \textit{CNSim} is evaluated on two dragonfly topologies with 1K and 16K nodes. The injection rate is set to 0.5 flits/cycle/node, and the simulation continues for 100K cycles. Each thread is issued 1/10/100 packets from the packet queue every time. As shown in Figure~\ref{chap06:fig:multi-threads}, the parallelism is insignificant for small-scale networks. About $50\%$ run time is saved using 32 100-issue threads for the 1K nodes \textit{Dragonfly}. However, multi-threading achieves significant speedup for larger-scale networks. The 4-thread and 32-thread 100-issue \textit{CNSim} are about $3\times$ and $5\times$ faster than the single-thread simulator for the 16K nodes \textit{Dragonfly}. Compared with the single-issue, the multiple-issue scheme reduces the thread contention among multiple workers, thus achieving better speedup. For the dragonfly topology, issuing a few packets each time is enough, and a larger issue number has limited improvement. If the processing runtime of one packet is smaller (\textit{e.g.}, by using simpler topologies and routing), the issue number should be increased.

As discussed in Section~\ref{chap06:sec:packet-parallel}, multi-threading and multi-issue without strict synchronization can lead to inconsistency. Therefore, the parallel inconsistency of \textit{CNSim} is evaluated on the same two \textit{Dragonfly} networks. The average latency of single-thread simulation and multi-thread simulation is measured and compared. All experiments use the same random number seed (\textit{i.e.}, and all packets are injected in the same order and timing) to eliminate random bias. As shown in Figure~\ref{chap06:fig:deviation}, the latency deviation of multithreading of most conditions is less than $0.2\%$. When the traffic is heavy, the inconsistency is slightly higher, but the deviation is still less than $0.5\%$. For the 1K nodes \textit{Dragonfly}, a larger issue number may lead to worse inconsistency, but for the 16K nodes \textit{Dragonfly}, the inconsistency is not significantly affected by the issue number. That is because packets are broadly distributed in large-scale networks, and the possibility of inverted allocation is low. In most cases, 10-issue and 16-thread simulations can achieve considerable speedup while maintaining high consistency.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{../figures/2024ATC/deviation.pdf}
    \caption{Average latency deviations of multi-threading and multiple-issue. \label{chap06:fig:deviation}}
\end{figure}

\subsection{Evaluation of Chiplet-based Networks}
In the previous practices, we have encountered two significant real problems. \textbf{1)} Current simulators cannot simulate the heterogeneity of the chiplet-based networks. \textbf{2)} The simulation speed is too slow to evaluate large-scale chiplet-based networks. \textit{CNSim} addresses the limitations of existing tools, making efficient and accurate evaluations possible. In this section, evaluations of the heterogeneous-link-based 2D-mesh/torus with billions of \textit{PARSEC} traces are present. Excluding the wasted and unshown experiments, about $500$ billion cycles and $20$ billion packets are simulated for the presented evaluations. The total simulation runtime is more than 20 hours, which is expected to take more than 200 hours if using other simulators.

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.7\linewidth]{../figures/2024ATC/hetero-link.pdf}
    \caption{Hetero-link-based chiplet architecture. (a) Traditional 2D-mesh on a single chip. (b) Chiplet-based 2D-mesh topology. (c) Hetero-link-based 2D-torus topology. \label{chap06:fig:hetero-link}}
\end{figure}

% \subsubsection{Setup}
As shown in Figure~\ref{chap06:fig:hetero-link}(b)(c), 2D-mesh and 2D-torus are two typical topologies for chiplet-based networks. All links are configured by two parameters: bandwidth (B) and latency (L). The bandwidth of the on-chip link is $1$ flit/cycle, and the latency is $1$ cycle. In the evaluation, two die-to-die configurations are used for chiplet-based networks: the low-latency parallel link (B=1, L=2) and the high-bandwidth serial link (B=2, L=4). Two routing algorithms are used for 2D-mesh: dimensional order (XY) routing and Duato-protocol-based negative-first adaptive routing (NFR)~\cite{Duato_InterconnectionNetworksEngineering_2003}; and the \textit{CLUE}~\cite{Luo_EfficientAdaptiveDeadlockFree_2012} routing algorithm is used for 2D-torus. Hetero-link is used in the chiplet-based 2D-torus topology, \textit{i.e.}, the adjacent die-to-die links are configured with (B=2, L=4), and the wraparound die-to-die links are configured with (B=2, L=4). 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\linewidth]{../figures/2024ATC/hetero-link-parsec.pdf}
    \caption{Evaluation results of PARSEC traces on chiplet-based networks. \label{chap06:fig:hetero-link-parsec}}
\end{figure}


\textit{PARSEC} traces are evaluated, 16 Bytes link width is used(\textit{i.e.}, the largest packet has $5$ flits), and all messages are injected according to the trace timestamps, even if queuing occurs. As shown in Figure~\ref{chap06:fig:hetero-link-parsec}, large-scale networks ($8\times 8$ nodes) with different configurations are evaluated. According to the results, the higher-bandwidth serial link is not beneficial since the bottleneck is on the on-chip links, which is consistent with the uniform traffic pattern. The deterministic XY routing algorithm for some workloads (\textit{e.g. Blackâ€“Scholes} and \textit{Fluidanimate}) is severely congested, and the average latency is much higher than the static network diameter. If the adaptive routing algorithm is used, the average latency of all workloads can be significantly reduced. The hetero-link-based 2D-torus with \textit{CLUE} routing algorithm achieves the lowest average latency for all workloads and almost completely eliminates network bottlenecks. The entire \textit{PARSEC} traces include over $100$ billion cycles and $3$ billion messages. Each round of experiments (some not presented in the chapter) is run for $\sim 2$ hours, which is quite efficient.

\section{Summary}

This chapter presents \textit{CNSim}, a cycle-accurate parallel network simulator designed for large-scale chiplet-based (shared-memory) networks. By using a packet-centric architecture and a novel atomic-based packet-parallel scheme, \textit{CNSim} achieves high simulation speed and scalability, about $11\times \sim 14\times$ faster than other cycle-accurate simulators. Based on \textit{CNSim} extensive evaluations on hetero-link-based networks and chiplet-based dragonfly networks are presented. The \textit{CNSim} and the evaluation framework are open-sourced to the community.